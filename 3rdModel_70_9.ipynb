{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1dsMyl635K8-"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import pandas as pd\n",
        "from tensorflow.keras import layers\n",
        "from glob import glob\n",
        "import gc"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install timm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SFJ4NbOg5L1g",
        "outputId": "55d44341-ce57-43cd-a96d-ddc4bee98d25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting timm\n",
            "  Downloading timm-0.5.4-py3-none-any.whl (431 kB)\n",
            "\u001b[?25l\r\u001b[K     |▊                               | 10 kB 25.2 MB/s eta 0:00:01\r\u001b[K     |█▌                              | 20 kB 32.7 MB/s eta 0:00:01\r\u001b[K     |██▎                             | 30 kB 21.7 MB/s eta 0:00:01\r\u001b[K     |███                             | 40 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 51 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |████▋                           | 61 kB 10.3 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 71 kB 10.4 MB/s eta 0:00:01\r\u001b[K     |██████                          | 81 kB 10.6 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 92 kB 11.8 MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 102 kB 9.7 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 112 kB 9.7 MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 122 kB 9.7 MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 133 kB 9.7 MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 143 kB 9.7 MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 153 kB 9.7 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 163 kB 9.7 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 174 kB 9.7 MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 184 kB 9.7 MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 194 kB 9.7 MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 204 kB 9.7 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 215 kB 9.7 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 225 kB 9.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 235 kB 9.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 245 kB 9.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 256 kB 9.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 266 kB 9.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 276 kB 9.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 286 kB 9.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 296 kB 9.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 307 kB 9.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 317 kB 9.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 327 kB 9.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 337 kB 9.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 348 kB 9.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 358 kB 9.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 368 kB 9.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 378 kB 9.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 389 kB 9.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 399 kB 9.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 409 kB 9.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 419 kB 9.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 430 kB 9.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 431 kB 9.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.4 in /usr/local/lib/python3.7/dist-packages (from timm) (1.10.0+cu111)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from timm) (0.11.1+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4->timm) (3.10.0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (1.21.5)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (7.1.2)\n",
            "Installing collected packages: timm\n",
            "Successfully installed timm-0.5.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from glob import glob\n",
        "import pandas as pd\n",
        "import numpy as np \n",
        "from tqdm import tqdm\n",
        "import cv2\n",
        "\n",
        "import os\n",
        "import timm\n",
        "import random\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "import time\n",
        "from PIL import Image "
      ],
      "metadata": {
        "id": "e3pfm6q75L3t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((512,512)),\n",
        "    transforms.RandomPerspective(0.5, 0.2),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomResizedCrop(size=400),\n",
        "    transforms.RandomRotation(90, expand=False),\n",
        "    transforms.ToTensor()\n",
        "])\n"
      ],
      "metadata": {
        "id": "8OohXVED5L5x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hZxxF14X5L9k",
        "outputId": "b660ca74-aceb-4ddd-bf51-aa88377da422"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train = sorted(glob('/content/gdrive/My Drive/Colab/train/train/*.png'))\n",
        "test = sorted(glob('/content/gdrive/MyDrive/Colab/test/test/*.png'))"
      ],
      "metadata": {
        "id": "kBnpVko-5L_r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_y = pd.read_csv(\"/content/gdrive/My Drive/Colab/train_df.csv\")"
      ],
      "metadata": {
        "id": "z2nwgZbx5MBx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_labels = train_y[\"label\"]\n",
        "\n",
        "label_unique = sorted(np.unique(train_labels))\n",
        "label_unique = {key:value for key,value in zip(label_unique, range(len(label_unique)))}\n",
        "\n",
        "train_labels = [label_unique[k] for k in train_labels]"
      ],
      "metadata": {
        "id": "RQQ75K2l5MD3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "idDrBAcxtkmv",
        "outputId": "f81bcbd9-b486-4d06-9c8a-ec678dd045e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4277"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Custom_dataset(Dataset):\n",
        "    def __init__(self, img_paths, labels, mode='train', transform=None):\n",
        "        self.img_paths = img_paths\n",
        "        self.labels = labels\n",
        "        self.mode = mode\n",
        "        self.transform = transform\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.img_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.img_paths[idx] \n",
        "        img = Image.open(img_path)\n",
        "\n",
        "        if self.transform is not None: \n",
        "            if self.mode=='train':               \n",
        "                img = transform(img)\n",
        "                if img.size()[0] ==1 :\n",
        "                  img = torch.stack((img,img,img)).squeeze()\n",
        "               \n",
        "        else:\n",
        "            if self.mode=='test':\n",
        "                transformm = transforms.Compose([\n",
        "                    transforms.Resize((512,512)),\n",
        "                    transforms.ToTensor(),\n",
        "                ])\n",
        "                img = transformm(img)\n",
        "                if img.size()[0] ==1 :\n",
        "                  img = torch.stack((img,img,img)).squeeze()\n",
        "\n",
        "        label = self.labels[idx]\n",
        "        return img, label\n",
        "\n",
        "class Network(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Network, self).__init__()\n",
        "        self.model = timm.create_model('efficientnet_b0', pretrained=True, num_classes=88)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.model(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "H5kFF1y55MFz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "epochs = 3\n",
        "\n",
        "# Train\n",
        "train_dataset = Custom_dataset(np.array(train), np.array(train_labels), mode='train', transform=transform)\n",
        "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\n",
        "\n",
        "# Test\n",
        "test_dataset = Custom_dataset(np.array(test), np.array([\"tmp\"]*len(test)), mode='test')\n",
        "test_loader = DataLoader(test_dataset, shuffle=False, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "l2l08UYH5MH5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()\n",
        "device = torch.device('cuda')\n",
        "net = Network().to(\"cuda:0\")"
      ],
      "metadata": {
        "id": "ODpMdSjJ5MKG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def score_function(real, pred):\n",
        "    score = f1_score(real, pred, average=\"macro\")\n",
        "    return score\n",
        "\n",
        "model = Network().to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "scaler = torch.cuda.amp.GradScaler() \n",
        "\n",
        "\n",
        "\n",
        "best=0\n",
        "epochs = 25\n",
        "for epoch in range(epochs):\n",
        "    start=time.time()\n",
        "    train_loss = 0\n",
        "    train_pred=[]\n",
        "    train_y=[]\n",
        "    model.train()\n",
        "    for batch in (train_loader):\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        x = torch.tensor(batch[0], dtype=torch.float32, device=device)\n",
        "        y = torch.tensor(batch[1], dtype=torch.long, device=device)\n",
        "        with torch.cuda.amp.autocast():\n",
        "            pred = model(x)\n",
        "        loss = criterion(pred, y)\n",
        "\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        \n",
        "        train_loss += loss.item()/len(train_loader)\n",
        "        train_pred += pred.argmax(1).detach().cpu().numpy().tolist()\n",
        "        train_y += y.detach().cpu().numpy().tolist()\n",
        "        \n",
        "    \n",
        "    train_f1 = score_function(train_y, train_pred)\n",
        "\n",
        "    TIME = time.time() - start\n",
        "    print(f'epoch : {epoch+1}/{epochs}    time : {TIME:.0f}s/{TIME*(epochs-epoch-1):.0f}s')\n",
        "    print(f'TRAIN    loss : {train_loss:.5f}    f1 : {train_f1:.5f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IFH-xceH5ML8",
        "outputId": "774c6684-fdf4-4318-f91e-1bc9a4fd2d08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch : 1/25    time : 279s/6698s\n",
            "TRAIN    loss : 1.21894    f1 : 0.16224\n",
            "epoch : 2/25    time : 258s/5944s\n",
            "TRAIN    loss : 0.76358    f1 : 0.25753\n",
            "epoch : 3/25    time : 258s/5678s\n",
            "TRAIN    loss : 0.65116    f1 : 0.34094\n",
            "epoch : 4/25    time : 259s/5439s\n",
            "TRAIN    loss : 0.60641    f1 : 0.33786\n",
            "epoch : 5/25    time : 258s/5164s\n",
            "TRAIN    loss : 0.58085    f1 : 0.36299\n",
            "epoch : 6/25    time : 260s/4936s\n",
            "TRAIN    loss : 0.53569    f1 : 0.39167\n",
            "epoch : 7/25    time : 261s/4704s\n",
            "TRAIN    loss : 0.50039    f1 : 0.43289\n",
            "epoch : 8/25    time : 264s/4487s\n",
            "TRAIN    loss : 0.47557    f1 : 0.46627\n",
            "epoch : 9/25    time : 255s/4084s\n",
            "TRAIN    loss : 0.49175    f1 : 0.44593\n",
            "epoch : 10/25    time : 255s/3828s\n",
            "TRAIN    loss : 0.45456    f1 : 0.47877\n",
            "epoch : 11/25    time : 255s/3569s\n",
            "TRAIN    loss : 0.43245    f1 : 0.50414\n",
            "epoch : 12/25    time : 256s/3322s\n",
            "TRAIN    loss : 0.41802    f1 : 0.51655\n",
            "epoch : 13/25    time : 255s/3060s\n",
            "TRAIN    loss : 0.40393    f1 : 0.53200\n",
            "epoch : 14/25    time : 254s/2799s\n",
            "TRAIN    loss : 0.40158    f1 : 0.53854\n",
            "epoch : 15/25    time : 255s/2550s\n",
            "TRAIN    loss : 0.39400    f1 : 0.55589\n",
            "epoch : 16/25    time : 254s/2289s\n",
            "TRAIN    loss : 0.39190    f1 : 0.54301\n",
            "epoch : 17/25    time : 255s/2040s\n",
            "TRAIN    loss : 0.36936    f1 : 0.55927\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "f_pred = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in (test_loader):\n",
        "        x = torch.tensor(batch[0], dtype = torch.float32, device = device)\n",
        "        with torch.cuda.amp.autocast():\n",
        "            pred = model(x)\n",
        "        f_pred.extend(pred.argmax(1).detach().cpu().numpy().tolist())\n",
        "\n",
        "label_decoder = {val:key for key, val in label_unique.items()}\n",
        "\n",
        "f_result = [label_decoder[result] for result in f_pred]"
      ],
      "metadata": {
        "id": "encdRv8G5MNw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submission = pd.read_csv(\"/content/gdrive/MyDrive/anomaly/sample_submission.csv\")\n",
        "\n",
        "submission[\"label\"] = f_result\n",
        "\n",
        "submission"
      ],
      "metadata": {
        "id": "IodRhMTx5MPp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submission.to_csv(\"4th_result.csv\", index = False)"
      ],
      "metadata": {
        "id": "0H0kOaQU5MR9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UgAtxGKd5ppI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}